{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso Regression  ( L1 Regularization )\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "u93QB4H8gs0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING DATA SCIENCE LIBRARIES"
      ],
      "metadata": {
        "id": "I2xjrloHcxJQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF9RMWnFZYq3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORT MACHINE LEARNING LIBRARIES AND CLASSES"
      ],
      "metadata": {
        "id": "-FDeOVL3M7rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score                 # for accuracy score\n",
        "from sklearn.model_selection import cross_val_score        # for cross validation score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression           # Import the LinearRegression class\n",
        "from sklearn.metrics import mean_squared_error, r2_score    # to find out the error functions\n",
        "from sklearn.preprocessing import PolynomialFeatures , StandardScaler   # for the polunomial features\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge   # ridge Regression\n",
        "from sklearn.linear_model import Lasso   # lasso Regression\n",
        "from sklearn.linear_model import ElasticNet   # elasticNet Regression\n",
        "from mlxtend.plotting import plot_linear_regression    # MLX bs\n",
        "from sklearn.linear_model import SGDRegressor"
      ],
      "metadata": {
        "id": "IVy_6tYPLJMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlx"
      ],
      "metadata": {
        "id": "7EdpU_ELM64n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. How are coefficients affected ?\n"
      ],
      "metadata": {
        "id": "XTPRT3iy9EuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_diabetes()\n",
        "df = pd.DataFrame(data.data , columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SLd4pZ_8YzEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(df.drop('target' , axis=1) , df['target'] , test_size=0.2 , random_state=2)"
      ],
      "metadata": {
        "id": "bpRbTs5tY6PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Performing Lasso Regression"
      ],
      "metadata": {
        "id": "zJIeZIrfZGr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coef = []\n",
        "r2_scores = []\n",
        "\n",
        "for i in [0, 0.1, 1, 10]:\n",
        "    reg = Lasso(alpha=i)\n",
        "    reg.fit(X_train, y_train)\n",
        "\n",
        "    coef.append(reg.coef_.tolist())\n",
        "    y_pred = reg.predict(X_test)\n",
        "    # Call the r2_score function from sklearn.metrics\n",
        "    r2_scores.append(r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Ray7_sZFZQN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLotting a graph for the Lasso Regression"
      ],
      "metadata": {
        "id": "SwMeFYQhaas2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 9))\n",
        "plt.subplot(221)\n",
        "plt.bar(data.feature_names, coef[0])\n",
        "plt.title('Alpha = 0 , r2_score{}'.format(round(r2_scores[0],2))  )\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 9))\n",
        "plt.subplot(222)\n",
        "plt.bar(data.feature_names, coef[1])\n",
        "plt.title('Alpha = 0.1 , r2_score{}'.format(round(r2_scores[1],2)) )\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 9))\n",
        "plt.subplot(224)\n",
        "plt.bar(data.feature_names, coef[2])\n",
        "plt.title('Alpha = 1 , r2_score{}'.format(round(r2_scores[2],2)) )\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 9))\n",
        "plt.subplot(224)\n",
        "plt.bar(data.feature_names, coef[3])\n",
        "plt.title('Alpha = 10 , r2_score{}'.format(round(r2_scores[3],2)))"
      ],
      "metadata": {
        "id": "tQtRCIhmaehU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When  we increase the value of Alpha too much that is we made it Alpha  = 10 then - it made all the weights of the features as 0  . This leads to Under-Fitting ."
      ],
      "metadata": {
        "id": "NRh_gG6YbY1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Higher Coefficients are Affected More"
      ],
      "metadata": {
        "id": "jmeys_zIeysu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = [0 , 0.001 , 0.01 , 0.1 , 1 , 10 , 100 , 1000]  # the different alpha values\n",
        "coefs = []         # We store our Coefficients here\n",
        "r2_scores = []      # We store our R2 Scores here\n",
        "\n",
        "for i in alphas:\n",
        "    reg = Ridge(alpha=i)\n",
        "    reg.fit(X_train, y_train)\n",
        "    coefs.append(reg.coef_.tolist())"
      ],
      "metadata": {
        "id": "Lzg2jy12e3jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_array  = np.array(coefs)\n",
        "\n",
        "coef_df = pd.DataFrame(input_array , columns=data.feature_names , index=alphas)\n",
        "coef_df['alpha'] = alphas\n",
        "coef_df.set_index('alpha')"
      ],
      "metadata": {
        "id": "_R8k5GDvfLIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Impact of Lamba on Bias and Variance"
      ],
      "metadata": {
        "id": "Wq4YorS9AlZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 100\n",
        "X = 5 * np.random.rand(m, 1)\n",
        "y = 0.7  + X**2 - X*2 + 3 + np.random.randn(m, 1)\n",
        "\n",
        "plt.scatter(X , y )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sOoi9LcbAxqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=2)"
      ],
      "metadata": {
        "id": "zMRCmaR-KOAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "alphas  = np.linspace( 0 , 30 , 100)\n",
        "\n",
        "loss = []\n",
        "bias = []\n",
        "variance = []\n",
        "\n",
        "for i in alphas:\n",
        "    reg = Lasso(alpha=i)\n",
        "\n",
        "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
        "        reg, X_train, y_train, X_test, y_test, loss='mse', random_seed=123 )\n",
        "\n",
        "    loss.append(avg_expected_loss)\n",
        "    bias.append(avg_bias)\n",
        "    variance.append(avg_var)"
      ],
      "metadata": {
        "id": "NlNTr1m2KSnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(alphas , loss , label='Loss')\n",
        "plt.plot(alphas , bias , label='Bias')\n",
        "plt.plot(alphas , variance , label='Variance')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYXuRW01NbtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Effect of Regularization on LOSS FUNCTION"
      ],
      "metadata": {
        "id": "GstPjWpWNl0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "x , y = make_regression(n_samples=100 , n_features=1 , n_informative=1 , n_targets=1 , noise=20 , random_state=13 )\n",
        "\n",
        "plt.scatter(x,  y)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x, y)\n",
        "print(lin_reg.coef_)\n",
        "print(lin_reg.intercept_)"
      ],
      "metadata": {
        "id": "9bte5459OPP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_loss ( m , alpha ):\n",
        "  return np.sum((y - m*x.ravel() + 2.29 )**2) + alpha*abs(m)"
      ],
      "metadata": {
        "id": "lV2AqVJDOUOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(m):\n",
        "  return m*x - 2.29"
      ],
      "metadata": {
        "id": "QcC591nHOiIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m  = np.linspace ( -45 , 100 , 100 )\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "for j in [0 , 10 , 100 ,  500, 1000, 2500 ]:\n",
        "    loss = []\n",
        "    for i  in range (m.shape[0]):\n",
        "\n",
        "      loss_i = cal_loss(m[i] , j)\n",
        "\n",
        "      loss.append(loss_i)\n",
        "    plt.plot(m , loss , label=f'alpha={j}')\n",
        "plt.legend()\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2jJPUnzTOusS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}